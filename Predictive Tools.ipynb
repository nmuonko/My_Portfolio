{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuSA+sXcLtiLhWCbdnnILt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nmuonko/my_portfolio/blob/main/Predictive%20Tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a Flask app that allows users to upload CSV files, which are saved to the uploads directory and parsed using the Python csv module. The parsed data is then sent to Elasticsearch via Logstash, and the top 10 log messages within a specified time range are displayed in the results.html template.\n",
        "\n",
        "The send_to_logstash() function reads in the CSV file using Python's csv.reader() function, and creates a dictionary for each row of data using the column headers as keys. The dictionary is then sent to Logstash using the logstash library.\n",
        "\n",
        "The get_results() function connects to Elasticsearch and retrieves the top 10 log messages within a specified time range using a search query. Note that because the data is now in a dictionary format, the get_results() function returns the entire dictionary for each log message, not just the message string.\n",
        "\n",
        "With this code, users can easily upload and analyze CSV log data, and identify potential issues or failures in their log data"
      ],
      "metadata": {
        "id": "ScBP11pAnCmz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64d1RwMDmrOC"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, render_template, request\n",
        "import os\n",
        "import json\n",
        "import socket\n",
        "import logstash\n",
        "from elasticsearch import Elasticsearch\n",
        "import csv\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/upload', methods=['POST'])\n",
        "def upload():\n",
        "    # get uploaded file\n",
        "    file = request.files['file']\n",
        "\n",
        "    # save file to disk\n",
        "    file_path = os.path.join('uploads', file.filename)\n",
        "    file.save(file_path)\n",
        "\n",
        "    # parse file and send data to Elasticsearch via Logstash\n",
        "    send_to_logstash(file_path)\n",
        "\n",
        "    # display results in web UI\n",
        "    results = get_results()\n",
        "    return render_template('results.html', results=results)\n",
        "\n",
        "def send_to_logstash(file_path):\n",
        "    # connect to Logstash\n",
        "    logger = logstash.TCPLogstashHandler('localhost', 5044, version=1)\n",
        "\n",
        "    # read in CSV file\n",
        "    with open(file_path, 'r') as f:\n",
        "        csv_reader = csv.reader(f)\n",
        "        headers = next(csv_reader)\n",
        "        for row in csv_reader:\n",
        "            log_dict = dict(zip(headers, row))\n",
        "            log_dict['source'] = file_path\n",
        "            log_dict['host'] = socket.gethostname()\n",
        "            logger.emit(json.dumps(log_dict))\n",
        "\n",
        "def get_results():\n",
        "    # connect to Elasticsearch\n",
        "    es = Elasticsearch()\n",
        "\n",
        "    # specify index name and time range\n",
        "    index_name = 'logstash-*'\n",
        "    start_time = 'now-7d'\n",
        "    end_time = 'now'\n",
        "\n",
        "    # define search query\n",
        "    query = {\n",
        "        \"query\": {\"range\": {\"@timestamp\": {\"gte\": start_time, \"lte\": end_time}}},\n",
        "        \"size\": 10000\n",
        "    }\n",
        "\n",
        "    # execute search query and get results\n",
        "    res = es.search(index=index_name, body=query)\n",
        "\n",
        "    # return top 10 log messages\n",
        "    return [hit['_source'] for hit in res['hits']['hits']][:10]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine learning-based predictive analytics tool**: I first read in the data from a CSV file using pandas. We then split the data into training and test sets using the train_test_split() function from scikit-learn. Next, we select a support vector machine (SVM) algorithm for our predictive model and use scikit-learn's Pipeline and GridSearchCV classes to tune the hyperparameters of the SVM.\n",
        "\n",
        "I evaalute the model's performance using cross-validation and the classification_report() function. Finally, we deploy the trained model in a production environment and monitor its performance over time.\n",
        "\n",
        "The below code is an example of how to build a machine learning-based predictive analytics tool.  This can help organizations to better understand and manage their data, and to make more informed decisions about potential failures or issues."
      ],
      "metadata": {
        "id": "zHE-8qq5oY3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Data Collection and Preparation\n",
        "data = pd.read_csv('data.csv')\n",
        "X = data.drop('label', axis=1)\n",
        "y = data['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Feature Engineering\n",
        "# Perform feature engineering as necessary\n",
        "\n",
        "# Step 3: Model Selection\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC())\n",
        "])\n",
        "param_grid = {\n",
        "    'svm__C': [0.1, 1, 10],\n",
        "    'svm__kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best parameters: \", grid_search.best_params_)\n",
        "\n",
        "# Step 4: Model Tuning\n",
        "# Perform model tuning as necessary\n",
        "\n",
        "# Step 5: Model Training and Evaluation\n",
        "clf = SVC(C=grid_search.best_params_['svm__C'], kernel=grid_search.best_params_['svm__kernel'])\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 6: Model Deployment and Monitoring\n",
        "# Deploy the trained model in a production environment and monitor its performance over time\n"
      ],
      "metadata": {
        "id": "xTv9G6LdoYkL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}